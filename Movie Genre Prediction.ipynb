{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Genre Prediction\n",
    "This project involves building a text classification model to predict movie genres from plot summaries. By leveraging NLP techniques, the text data is preprocessed and transformed into a format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\youse\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\youse\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\youse\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\youse\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\youse\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "   ---------------------------------------- 0.0/258.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/258.0 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/258.0 kB 435.7 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/258.0 kB 871.5 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 204.8/258.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.0/258.0 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "train_data = pd.read_parquet(r\"C:\\Users\\youse\\Desktop\\CodeClause\\archive (5)\\train-00000-of-00001-b943ea66e0040b18.parquet\")\n",
    "test_data = pd.read_parquet(r\"C:\\Users\\youse\\Desktop\\CodeClause\\archive (5)\\test-00000-of-00001-35e9a9274361daed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44978</td>\n",
       "      <td>Super Me</td>\n",
       "      <td>A young scriptwriter starts bringing valuable ...</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50185</td>\n",
       "      <td>Entity Project</td>\n",
       "      <td>A director and her friends renting a haunted h...</td>\n",
       "      <td>horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34131</td>\n",
       "      <td>Behavioral Family Therapy for Serious Psychiat...</td>\n",
       "      <td>This is an educational video for families and ...</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78522</td>\n",
       "      <td>Blood Glacier</td>\n",
       "      <td>Scientists working in the Austrian Alps discov...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2206</td>\n",
       "      <td>Apat na anino</td>\n",
       "      <td>Buy Day - Four Men Widely - Apart in Life - By...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         movie_name  \\\n",
       "0  44978                                           Super Me   \n",
       "1  50185                                     Entity Project   \n",
       "2  34131  Behavioral Family Therapy for Serious Psychiat...   \n",
       "3  78522                                      Blood Glacier   \n",
       "4   2206                                      Apat na anino   \n",
       "\n",
       "                                            synopsis    genre  \n",
       "0  A young scriptwriter starts bringing valuable ...  fantasy  \n",
       "1  A director and her friends renting a haunted h...   horror  \n",
       "2  This is an educational video for families and ...   family  \n",
       "3  Scientists working in the Austrian Alps discov...    scifi  \n",
       "4  Buy Day - Four Men Widely - Apart in Life - By...   action  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16863</td>\n",
       "      <td>A Death Sentence</td>\n",
       "      <td>12 y.o. Ida's dad'll die without a DKK1,500,00...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48456</td>\n",
       "      <td>Intermedio</td>\n",
       "      <td>A group of four teenage friends become trapped...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41383</td>\n",
       "      <td>30 Chua Phai Tet</td>\n",
       "      <td>A guy left his home for 12 years till he came ...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84007</td>\n",
       "      <td>Paranoiac</td>\n",
       "      <td>A man long believed dead returns to the family...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40269</td>\n",
       "      <td>Ordinary Happiness</td>\n",
       "      <td>After a deadly accident, Paolo comes back on E...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id          movie_name  \\\n",
       "0  16863    A Death Sentence   \n",
       "1  48456          Intermedio   \n",
       "2  41383    30 Chua Phai Tet   \n",
       "3  84007           Paranoiac   \n",
       "4  40269  Ordinary Happiness   \n",
       "\n",
       "                                            synopsis   genre  \n",
       "0  12 y.o. Ida's dad'll die without a DKK1,500,00...  action  \n",
       "1  A group of four teenage friends become trapped...  action  \n",
       "2  A guy left his home for 12 years till he came ...  action  \n",
       "3  A man long believed dead returns to the family...  action  \n",
       "4  After a deadly accident, Paolo comes back on E...  action  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the datasets\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'movie_name', 'synopsis', 'genre'], dtype='object')\n",
      "Index(['id', 'movie_name', 'synopsis', 'genre'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the columns of the datasets\n",
    "print(train_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in train_data:\n",
      "id            0\n",
      "movie_name    0\n",
      "synopsis      0\n",
      "genre         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in train_data\n",
    "null_train = train_data.isnull().sum()\n",
    "print(\"Null values in train_data:\")\n",
    "print(null_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values in test_data:\n",
      "id            0\n",
      "movie_name    0\n",
      "synopsis      0\n",
      "genre         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in test_data\n",
    "null_test = test_data.isnull().sum()\n",
    "print(\"\\nNull values in test_data:\")\n",
    "print(null_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates in train_data: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in train_data\n",
    "duplicate_train = train_data.duplicated().sum()\n",
    "print(\"\\nDuplicates in train_data:\", duplicate_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in test_data: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in test_data\n",
    "duplicate_test = test_data.duplicated().sum()\n",
    "print(\"Duplicates in test_data:\", duplicate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\youse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Lowercase conversion\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# Load train and test datasets\n",
    "train_data = pd.read_parquet(r\"C:\\Users\\youse\\Desktop\\CodeClause\\archive (5)\\train-00000-of-00001-b943ea66e0040b18.parquet\")\n",
    "test_data = pd.read_parquet(r\"C:\\Users\\youse\\Desktop\\CodeClause\\archive (5)\\test-00000-of-00001-35e9a9274361daed.parquet\")\n",
    "\n",
    "# Apply preprocessing to synopsis text in train and test datasets\n",
    "train_data['cleaned_synopsis'] = train_data['synopsis'].apply(preprocess_text)\n",
    "test_data['cleaned_synopsis'] = test_data['synopsis'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "fantasy      5400\n",
      "horror       5400\n",
      "family       5400\n",
      "scifi        5400\n",
      "action       5400\n",
      "crime        5400\n",
      "adventure    5400\n",
      "mystery      5400\n",
      "romance      5400\n",
      "thriller     5400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(train_data['cleaned_synopsis'])\n",
    "y = train_data['genre']\n",
    "\n",
    "# Check class distribution\n",
    "print(train_data['genre'].value_counts())\n",
    "\n",
    "# Use SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34879629629629627\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      action       0.28      0.25      0.26      1094\n",
      "   adventure       0.27      0.23      0.25      1067\n",
      "       crime       0.37      0.39      0.38      1134\n",
      "      family       0.39      0.45      0.42      1049\n",
      "     fantasy       0.30      0.28      0.29      1057\n",
      "      horror       0.40      0.42      0.41      1116\n",
      "     mystery       0.29      0.28      0.29      1074\n",
      "     romance       0.48      0.58      0.52      1075\n",
      "       scifi       0.39      0.44      0.41      1077\n",
      "    thriller       0.21      0.16      0.19      1057\n",
      "\n",
      "    accuracy                           0.35     10800\n",
      "   macro avg       0.34      0.35      0.34     10800\n",
      "weighted avg       0.34      0.35      0.34     10800\n",
      "\n",
      "Confusion Matrix:\n",
      " [[271 139 198  73  69  51  50  59 106  78]\n",
      " [127 250  57 154 133  40  42  76 142  46]\n",
      " [156  47 437  47  21  39 150  92  22 123]\n",
      " [ 48 103  37 471 124  21  39 130  52  24]\n",
      " [ 64 118  16 161 296  96  70  77 125  34]\n",
      " [ 42  43  38  47  92 473 137  37  99 108]\n",
      " [ 33  31 133  59  64 170 300  80  75 129]\n",
      " [ 33  56  68 102  44  23  43 624  35  47]\n",
      " [ 71  98  23  59 101 108  53  42 471  51]\n",
      " [112  46 177  39  36 167 140  95  71 174]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "print('Accuracy:', accuracy_score(y_val, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_val, y_pred, zero_division=1))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0885\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      action       1.00      0.09      0.16     36000\n",
      "   adventure       0.00      1.00      0.00         0\n",
      "       crime       0.00      1.00      0.00         0\n",
      "      family       0.00      1.00      0.00         0\n",
      "     fantasy       0.00      1.00      0.00         0\n",
      "      horror       0.00      1.00      0.00         0\n",
      "     mystery       0.00      1.00      0.00         0\n",
      "     romance       0.00      1.00      0.00         0\n",
      "       scifi       0.00      1.00      0.00         0\n",
      "    thriller       0.00      1.00      0.00         0\n",
      "\n",
      "    accuracy                           0.09     36000\n",
      "   macro avg       0.10      0.91      0.02     36000\n",
      "weighted avg       1.00      0.09      0.16     36000\n",
      "\n",
      "Test Confusion Matrix:\n",
      " [[3186 3144 3762 4101 3098 4002 3537 4400 4065 2705]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "X_test = vectorizer.transform(test_data['cleaned_synopsis'])\n",
    "y_test = test_data['genre']  # Assuming you have the actual genres for the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_test_pred))\n",
    "print('Test Classification Report:\\n', classification_report(y_test, y_test_pred, zero_division=1))\n",
    "print('Test Confusion Matrix:\\n', confusion_matrix(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
